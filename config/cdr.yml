####################
# Training params
###################
lr: 1.e-3
grad_clip: True
clip_value: 0.3
grad_clip_type: "value"
weight_decay: 0

# effective batch_size is batch_size X n_samples_from_each_video
batch_size: 256
local_video_sampling: True   # each batch contain n_samples_from_each_video
n_samples_from_each_video: 4

####################
# Evaluation params
###################
evaluate_texture_dependency: False
evaluate_geometric_distance: False

######################
# Domain Randomization
######################
contrastive_dr: True  # if ture use Contrastive Domain Randomization (use second texture as label) else use baseline
use_both_textures: True  # load both textures, if contrastive_dr is False use both textures as separate samples, for fair comparison with baseline

#####################
# Data Augmentations
#####################
use_data_aug: True
contrastive_data_aug: True  # if True, the positive label is applied with different random augmentation, else applied the same augmentation to both input and label observation
color_jitter_p: 0.2
gaussian_blur_p: 0.2
global_shift_p: 0.

######################
# Planning
######################
simulation_config_path: -1   # path to simulated data configs, relevant only if planning is performed during training for evaluation

###################
# Model params
##################
# general
similarity_function: "mse_similarity"  # can be either "dot_product_similarity", "mse_similarity" or "cosine_similarity"
z_dim: 16   # latent vector representation
action_dim: 4
n_neg_actions: 64  # number of negative samples generated by random actions as input instead of the correct action
temperature: 0.05   # scaling for the loss function
use_oracle_states: False  # use oracle state as observations instead of images and don't use encoder
image_size: 128

# forward_model (type can be linear, V1 or V2)
forward_model_name: "CFM"    # can be either "linear", "MLP", "CFM" or "projected"
action_hidden_dim: 64
z_hidden_dim: 64
z_delta: True   # is True, add a skip connection to the forward model, so the forward model learns only the change needed to add for predicting the next step

# encoder
encoder_type: "resnet18"    # can be either "resnet18", "ConvNet" or "LiteConvNet"

####################
# Dataset params
###################
train_n_videos: -1  # if set to -1, loads all video in directory
train_range: [0, 20] #[0, 2700]  # range of videos indexes to use, useful to separate train and validation data
val_n_videos: -1  # if set to -1, loads all video in directory
val_range: [20, 30]  #[2700, 3000]  # range of videos indexes to use, useful to separate train and validation data

# state action representation
action_files_sfx: "_state_action_label.npy"
second_video_sfx: "_2"
seg_masks_files_sfx: "_seg_mask.npy"